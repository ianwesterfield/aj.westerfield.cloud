networks:
  webtools_network:
    external: true

secrets:
  huggingface_pat:
    file: ./secrets/huggingface_pat.txt
  funnel_local_agent_host:
    file: ./secrets/funnel_local_agent_host.txt
  funnel_gossip_seed_host:
    file: ./secrets/funnel_gossip_seed_host.txt

services:
  # -------------------------------------------------
  # Ollama
  # -------------------------------------------------
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    network_mode: host
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_KEEP_ALIVE=-1 # Keep models loaded forever (never unload)
      - OLLAMA_NUM_PARALLEL=1 # CRITICAL: Only 1 concurrent request to prevent double-loads
      - OLLAMA_MAX_LOADED_MODELS=1 # CRITICAL: Maximum 1 model loaded at a time
    volumes:
      - /mnt/c/docker-data/ollama/.ollama:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    shm_size: 2g
    restart: unless-stopped
    # Pre-load llama3.1:8b on startup so chat is instant
    healthcheck:
      test:
        [
          "CMD-SHELL",
          'curl -sf http://localhost:11434/api/tags || exit 1; curl -sf -X POST http://localhost:11434/api/generate -d ''{"model":"llama3.1:8b","prompt":"","keep_alive":-1}'' > /dev/null 2>&1; exit 0',
        ]
      interval: 30s
      timeout: 300s
      retries: 3
      start_period: 30s

  # -------------------------------------------------
  # Ollama — Orchestrator (dedicated reasoning/planning)
  # Pinned to aj-deepseek-r1-32b so chat model swaps
  # don't cause 126-second reloads mid-trajectory
  # -------------------------------------------------
  ollama-orchestrator:
    image: ollama/ollama:latest
    container_name: ollama-orchestrator
    network_mode: host
    environment:
      - OLLAMA_HOST=0.0.0.0:11435
      - OLLAMA_KEEP_ALIVE=-1
      - OLLAMA_NUM_PARALLEL=2
      - OLLAMA_MAX_LOADED_MODELS=1
    volumes:
      # Share models with main ollama (custom models like aj-deepseek-r1-32b)
      - /mnt/c/docker-data/ollama/.ollama:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    shm_size: 2g
    restart: unless-stopped
    # Pre-load aj-deepseek-r1-32b so orchestrator reasoning is instant
    healthcheck:
      test:
        [
          "CMD-SHELL",
          'curl -sf http://localhost:11435/api/tags || exit 1; curl -sf -X POST http://localhost:11435/api/generate -d ''{ "model": "aj-deepseek-r1-32b", "prompt": "", "keep_alive": -1 }'' > /dev/null 2>&1; exit 0',
        ]
      interval: 30s
      timeout: 300s
      retries: 3
      start_period: 60s

  # -------------------------------------------------
  # Ollama — Fact Extraction (qwen2.5:1.5b ~1GB VRAM)
  # Tiny model for structured JSON key/value extraction
  # Used by pragmatics for memory fact extraction
  # -------------------------------------------------
  ollama-facts:
    image: ollama/ollama:latest
    container_name: ollama-facts
    network_mode: host
    environment:
      - OLLAMA_HOST=0.0.0.0:11436
      - OLLAMA_KEEP_ALIVE=-1
      - OLLAMA_NUM_PARALLEL=4 # Tiny model, can handle more concurrency
      - OLLAMA_MAX_LOADED_MODELS=1
    volumes:
      # Share models with main ollama (qwen2.5:1.5b needs to be pulled to main)
      - /mnt/c/docker-data/ollama/.ollama:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    shm_size: 512m
    restart: unless-stopped
    # Pre-load qwen2.5:1.5b so fact extraction is instant
    healthcheck:
      test:
        [
          "CMD-SHELL",
          'curl -sf http://localhost:11436/api/tags || exit 1; curl -sf -X POST http://localhost:11436/api/generate -d ''{ "model": "qwen2.5:1.5b", "prompt": "", "keep_alive": -1 }'' > /dev/null 2>&1; exit 0',
        ]
      interval: 30s
      timeout: 120s
      retries: 3
      start_period: 15s

  # -------------------------------------------------
  # Qdrant
  # -------------------------------------------------
  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    network_mode: host
    environment:
      - QDRANT__LOG_LEVEL=DEBUG
      # Flush WAL more frequently to reduce corruption risk on shutdown
      - QDRANT__STORAGE__OPTIMIZERS__FLUSH_INTERVAL_SEC=1
    volumes:
      - /mnt/c/docker-data/qdrant/storage:/qdrant/storage
    restart: unless-stopped
    # Give Qdrant time to flush WAL and close cleanly
    stop_grace_period: 30s

  # -------------------------------------------------
  # Memory API (Memory + Agent Core)
  # -------------------------------------------------
  memory_api:
    build:
      context: ./layers
      dockerfile: memory/Dockerfile
      args:
        BASE_REGISTRY: ${BASE_REGISTRY:-ianwesterfield}
    container_name: memory_api
    network_mode: host
    environment:
      - QDRANT_HOST=localhost
      - QDRANT_PORT=6333
      - INDEX_NAME=user_memory_collection
      - EMBEDDING_PROVIDER=sentence_transformers # Keep ST for embeddings (768-dim, fast)
      - OLLAMA_BASE_URL=http://localhost:11434
      # OLLAMA_MODEL is now passed dynamically from Open-WebUI selection
      - SUMMARY_MODEL=sshleifer/distilbart-cnn-12-6
      - SUMMARY_DEVICE=0
      - HF_HOME=/models
      - HF_TOKEN_FILE=/run/secrets/huggingface_pat
      - FILTERS_PATH=/filters
    secrets:
      - huggingface_pat
    volumes:
      - /mnt/c/docker-data/models:/models
      - ./filters:/filters:ro
    depends_on:
      - qdrant
    restart: unless-stopped

  # -------------------------------------------------
  # Pragmatics API
  # Intent classification: DistilBERT (CPU, baked into image)
  # Fact extraction: llama3.1:8b via dedicated ollama-facts
  # -------------------------------------------------
  pragmatics_api:
    build:
      context: ./layers/pragmatics
      dockerfile: Dockerfile
      args:
        BASE_REGISTRY: ${BASE_REGISTRY:-ianwesterfield}
    container_name: pragmatics_api
    network_mode: host
    environment:
      - OLLAMA_HOST=localhost
      - OLLAMA_PORT=11436
      - FACT_EXTRACTION_MODEL=qwen2.5:1.5b
    depends_on:
      - ollama-facts
    restart: unless-stopped

  # -------------------------------------------------
  # Extractor API (Media -> Text)
  # -------------------------------------------------
  extractor_api:
    build:
      context: ./layers/extractor
      dockerfile: Dockerfile
      args:
        BASE_REGISTRY: ${BASE_REGISTRY:-ianwesterfield}
    container_name: extractor_api
    network_mode: host
    environment:
      - WHISPER_MODEL=large-v3
      - IMAGE_MODEL=llava # Options: llava-4bit (~4GB), llava (~14GB), florence (~4GB)
      - HF_HOME=/models
      - HF_TOKEN_FILE=/run/secrets/huggingface_pat
    secrets:
      - huggingface_pat
    volumes:
      - /mnt/c/docker-data/models:/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped

  # -------------------------------------------------
  # Orchestrator API (Agentic Reasoning + Execution Engine)
  # Includes embedded FunnelCloud Agent for localhost bootstrap
  # -------------------------------------------------
  orchestrator_api:
    build:
      context: ./layers
      dockerfile: orchestrator/Dockerfile
    container_name: orchestrator_api
    network_mode: host
    environment:
      # Dedicated Ollama instance for orchestrator reasoning (port 11435)
      - OLLAMA_BASE_URL=http://localhost:11435
      - MEMORY_API_URL=http://localhost:8000
      - MAX_PARALLEL_TASKS=5
      - WORKSPACE_ROOT=/workspace
      - HOST_WORKSPACE_PATH=${HOST_WORKSPACE_PATH:-/mnt/c/Code}

      # Executor settings (now local)
      - DEFAULT_TIMEOUT=30
      - MAX_TIMEOUT=120

      # ====== Embedded FunnelCloud Agent ======
      # The orchestrator container runs its own FunnelCloud agent
      # This provides the localhost bootstrap for discover-peers
      - FUNNEL_AGENT_ID=orchestrator-agent
      - FUNNEL_INSECURE=${FUNNEL_INSECURE:-true}

      # FunnelCloud agent discovery
      - FUNNEL_DISCOVERY_PORT=41420
      - FUNNEL_GRPC_PORT=41235
      - FUNNEL_DISCOVERY_TIMEOUT=3.0
      # HTTP port for agent REST API (health, discover-peers)
      - FUNNEL_AGENT_HTTP_PORT=41421
      # Gossip config - how many rounds to expand peer discovery
      - FUNNEL_GOSSIP_MAX_ROUNDS=3
      - FUNNEL_GOSSIP_TIMEOUT=2.0
      # Gossip seed - ONE reachable agent IP to bootstrap cross-subnet discovery
      # (only needed when embedded agent can't multicast to other subnets)
      - FUNNEL_LOCAL_AGENT_HOST_FILE=/run/secrets/funnel_local_agent_host
      - FUNNEL_GOSSIP_SEED_HOST_FILE=/run/secrets/funnel_gossip_seed_host
      # mTLS certificates for FunnelCloud gRPC
      - ORCHESTRATOR_CERT_PATH=/app/certs/agents/orchestrator/agent.crt
      - ORCHESTRATOR_KEY_PATH=/app/certs/agents/orchestrator/agent.key
      - CA_CERT_PATH=/app/certs/ca/ca.crt
    secrets:
      - funnel_local_agent_host
      - funnel_gossip_seed_host
    volumes:
      - ${HOST_WORKSPACE_PATH:-/mnt/c/Code}:/workspace:rw
      - ./layers/agents/FunnelCloud/certs:/app/certs:ro
    depends_on:
      - ollama-orchestrator
      - memory_api
    restart: unless-stopped

  # -------------------------------------------------
  # Open-WebUI
  # -------------------------------------------------
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    depends_on:
      - ollama
    environment:
      - USER_AGENT=WesterfieldCloud-OpenWebUI/1.0
      - GLOBAL_LOG_LEVEL=DEBUG
      # All backend services are on host network - use Docker bridge gateway (172.17.0.1)
      # Note: host.docker.internal doesn't route to WSL2 host network in this setup
      - OLLAMA_BASE_URL=http://172.17.0.1:11434
      - ENABLE_API_KEY=true # Allow API key authentication
      - RAG_EMBEDDING_ENGINE=ollama
      - EMBEDDING_MODEL=BAAI/bge-large-en-v1.5
      - MEMORY_ENABLED=true
      - MEMORY_SHORT_TERM_MEMORY_DEPTH=15
      - TOOLS_FUNCTION_CALLING_ENABLED=true
      - TOOLS_FUNCTION_CALLING_LLMS=ollama
      - MEMORY_API_URL=http://172.17.0.1:8000
      - TOOLS_DISCOVERY_URL=http://172.17.0.1:8000/api/tools
      # Orchestrator for AJ filter task execution
      - ORCHESTRATOR_API_URL=http://172.17.0.1:8004
      # Pragmatics for AJ filter intent classification
      - PRAGMATICS_API_URL=http://172.17.0.1:8001
      # Extractor for AJ filter media extraction
      - EXTRACTOR_API_URL=http://172.17.0.1:8002
    ports:
      - "8180:8080" # Host 8180 -> Container 8080 (8080 often reserved by Windows/Hyper-V)
    volumes:
      - /mnt/c/docker-data/open-webui/data:/app/backend/data
    deploy:
      resources:
        limits:
          cpus: "4"
          memory: 8g
    restart: unless-stopped
    networks:
      - webtools_network
