networks:
  webtools_network:
    external: true

services:
  # -------------------------------------------------
  # Ollama
  # -------------------------------------------------
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    network_mode: host
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_KEEP_ALIVE=24h # Keep models loaded for 24 hours (ensures persistent loading)
      - OLLAMA_NUM_PARALLEL=1 # CRITICAL: Only 1 concurrent request to prevent double-loads
      - OLLAMA_MAX_LOADED_MODELS=1 # CRITICAL: Maximum 1 model loaded at a time
    volumes:
      - /mnt/c/docker-data/ollama/.ollama:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    shm_size: 2g
    restart: unless-stopped

  # -------------------------------------------------
  # Qdrant
  # -------------------------------------------------
  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    network_mode: host
    environment:
      - QDRANT__LOG_LEVEL=DEBUG
      # Flush WAL more frequently to reduce corruption risk on shutdown
      - QDRANT__STORAGE__OPTIMIZERS__FLUSH_INTERVAL_SEC=1
    volumes:
      - /mnt/c/docker-data/qdrant/storage:/qdrant/storage
    restart: unless-stopped
    # Give Qdrant time to flush WAL and close cleanly
    stop_grace_period: 30s

  # -------------------------------------------------
  # Memory API (Memory + Agent Core)
  # -------------------------------------------------
  memory_api:
    build:
      context: ./layers
      dockerfile: memory/Dockerfile
      args:
        BASE_REGISTRY: ${BASE_REGISTRY:-ianwesterfield}
    container_name: memory_api
    network_mode: host
    environment:
      - QDRANT_HOST=localhost
      - QDRANT_PORT=6333
      - INDEX_NAME=user_memory_collection
      - EMBEDDING_PROVIDER=sentence_transformers # Keep ST for embeddings (768-dim, fast)
      - OLLAMA_BASE_URL=http://localhost:11434
      # OLLAMA_MODEL is now passed dynamically from Open-WebUI selection
      - SUMMARY_MODEL=sshleifer/distilbart-cnn-12-6
      - SUMMARY_DEVICE=0
      - HF_HOME=/models
      - FILTERS_PATH=/filters
    volumes:
      - /mnt/c/docker-data/models:/models
      - ./filters:/filters:ro
    depends_on:
      - qdrant
    restart: unless-stopped

  # -------------------------------------------------
  # Pragmatics API (Intent Classification)
  # -------------------------------------------------
  pragmatics_api:
    build:
      context: ./layers/pragmatics
      dockerfile: Dockerfile
      args:
        BASE_REGISTRY: ${BASE_REGISTRY:-ianwesterfield}
    container_name: pragmatics_api
    network_mode: host
    environment:
      # Use 4-class intent model (casual/save/recall/task)
      # Fallback to distilbert_memory if distilbert_intent not found
      - CLASSIFIER_MODEL=distilbert_intent
      - INTENT_CONFIDENCE_THRESHOLD=0.50
    depends_on:
      - memory_api
    restart: unless-stopped

  # -------------------------------------------------
  # Extractor API (Media -> Text)
  # -------------------------------------------------
  extractor_api:
    build:
      context: ./layers/extractor
      dockerfile: Dockerfile
      args:
        BASE_REGISTRY: ${BASE_REGISTRY:-ianwesterfield}
    container_name: extractor_api
    network_mode: host
    environment:
      - WHISPER_MODEL=large-v3
      - IMAGE_MODEL=llava # Options: llava-4bit (~4GB), llava (~14GB), florence (~4GB)
      - HF_HOME=/models
    volumes:
      - /mnt/c/docker-data/models:/models
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped

  # -------------------------------------------------
  # Orchestrator API (Agentic Reasoning + Execution Engine)
  # -------------------------------------------------
  orchestrator_api:
    build:
      context: ./layers
      dockerfile: orchestrator/Dockerfile
    container_name: orchestrator_api
    network_mode: host
    environment:
      # Use localhost since we're on host network (Linux in WSL)
      - OLLAMA_BASE_URL=http://localhost:11434
      - MEMORY_API_URL=http://localhost:8000
      - MAX_PARALLEL_TASKS=5
      - WORKSPACE_ROOT=/workspace
      - HOST_WORKSPACE_PATH=${HOST_WORKSPACE_PATH:-/mnt/c/Code}

      # Executor settings (now local)
      - DEFAULT_TIMEOUT=30
      - MAX_TIMEOUT=120

      # FunnelCloud agent discovery
      # Local agent host for discovery proxy (UDP on same port as discovery)
      # This is the ONLY known address - the local agent does multicast on the physical LAN
      # All other agents are discovered dynamically via gossip (no hardcoding needed)
      - FUNNEL_LOCAL_AGENT_HOST=${FUNNEL_LOCAL_AGENT_HOST:-}
      - FUNNEL_DISCOVERY_PORT=41420
      - FUNNEL_GRPC_PORT=41235
      - FUNNEL_DISCOVERY_TIMEOUT=3.0
      # HTTP port for agent REST API (used for gossip peer discovery)
      - FUNNEL_AGENT_HTTP_PORT=41421
      # Gossip config - how many rounds to expand peer discovery
      - FUNNEL_GOSSIP_MAX_ROUNDS=3
      - FUNNEL_GOSSIP_TIMEOUT=2.0
      # Gossip seed - ONE reachable agent IP to bootstrap cross-subnet discovery
      # (only needed when local agent can't multicast to other subnets)
      - FUNNEL_GOSSIP_SEED_HOST=${FUNNEL_GOSSIP_SEED_HOST:-}
      # mTLS certificates for FunnelCloud gRPC
      - ORCHESTRATOR_CERT_PATH=/app/certs/agents/orchestrator/agent.crt
      - ORCHESTRATOR_KEY_PATH=/app/certs/agents/orchestrator/agent.key
      - CA_CERT_PATH=/app/certs/ca/ca.crt
      # Set to 'true' to skip mTLS (when agents don't have certs configured)
      - FUNNEL_INSECURE=${FUNNEL_INSECURE:-true}
    volumes:
      - ${HOST_WORKSPACE_PATH:-/mnt/c/Code}:/workspace:rw
      - ./layers/agents/FunnelCloud/certs:/app/certs:ro
    depends_on:
      - ollama
      - memory_api
    restart: unless-stopped

  # -------------------------------------------------
  # Open-WebUI
  # -------------------------------------------------
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    depends_on:
      - ollama
    environment:
      - USER_AGENT=WesterfieldCloud-OpenWebUI/1.0
      - GLOBAL_LOG_LEVEL=DEBUG
      # All backend services are on host network - use Docker bridge gateway (172.17.0.1)
      # Note: host.docker.internal doesn't route to WSL2 host network in this setup
      - OLLAMA_BASE_URL=http://172.17.0.1:11434
      - ENABLE_API_KEY=true # Allow API key authentication
      - RAG_EMBEDDING_ENGINE=ollama
      - EMBEDDING_MODEL=BAAI/bge-large-en-v1.5
      - MEMORY_ENABLED=true
      - MEMORY_SHORT_TERM_MEMORY_DEPTH=15
      - TOOLS_FUNCTION_CALLING_ENABLED=true
      - TOOLS_FUNCTION_CALLING_LLMS=ollama
      - MEMORY_API_URL=http://172.17.0.1:8000
      - TOOLS_DISCOVERY_URL=http://172.17.0.1:8000/api/tools
      # Orchestrator for AJ filter task execution
      - ORCHESTRATOR_API_URL=http://172.17.0.1:8004
      # Pragmatics for AJ filter intent classification
      - PRAGMATICS_API_URL=http://172.17.0.1:8001
      # Extractor for AJ filter media extraction
      - EXTRACTOR_API_URL=http://172.17.0.1:8002
    ports:
      - "8180:8080" # Host 8180 -> Container 8080 (8080 often reserved by Windows/Hyper-V)
    volumes:
      - /mnt/c/docker-data/open-webui/data:/app/backend/data
    deploy:
      resources:
        limits:
          cpus: "4"
          memory: 8g
    restart: unless-stopped
    networks:
      - webtools_network
