# Mixed Dataset v1 Training Config for H200 (141GB VRAM)
# AJ-DeepSeekR1Qwen32B-v2.1.0-lora
#
# Dataset: Mixed conversational + plant/farming + apothecary + strategic
# Focus: Conversational adaptability, domain knowledge, reasoning

# Model
model_name: "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"
output_dir: "./AJ-DeepSeekR1Qwen32B-v2.1.0-lora"

# Dataset (generated by prepare_mixed_v1.py)
train_file: "datasets/processed/mixed_v1.train.jsonl"
eval_file: "datasets/processed/mixed_v1.eval.jsonl"

# LoRA Configuration
# Same as v1.0 for A/B comparison
lora_r: 64
lora_alpha: 128
lora_dropout: 0.05
target_modules:
  - "q_proj"
  - "k_proj"
  - "v_proj"
  - "o_proj"
  - "gate_proj"
  - "up_proj"
  - "down_proj"

# Batch Size
# Effective batch size = batch_size * gradient_accumulation_steps = 16
batch_size: 2
gradient_accumulation_steps: 8

# Training Duration
# Mixed dataset is ~300K examples with 50% conversational + 40% domain + 10% strategic
# context_switching.jsonl weighted 3x for reliable contextType behavior
# 2 epochs for production-grade context switching reliability (~95-98%)
num_epochs: 2
max_steps: -1

# Optimizer
learning_rate: 2e-5
weight_decay: 0.01
warmup_steps: 100

# Sequence Length
max_seq_length: 2048

# Logging & Checkpoints
logging_steps: 10
save_steps: 500
eval_steps: 500

# Versioning
version: "v2.1.0-lora"
dataset_version: "mixed_v1"
base_model_code: "DeepSeekR1Qwen32B"

# In addition to the inherent DeepSeek-R1 capabilities, this configuration enhances:
# - 50% conversational patterns (WildChat 82K + UltraChat 45K from HuggingFace)
# - 40% domain knowledge (loaded from training/data/*.jsonl - ~26K examples):
#     * Development: Angular, TypeScript, Python, Node.js, .NET, API, Web fundamentals
#     * Infrastructure: Docker, Linux, Windows, Cloud/DevOps, Networking, Security
#     * Data: SQL, Qdrant vectors, Storage patterns
#     * Quality: Code review, Refactoring, Testing, Architecture
#     * Agentic: AJ persona, Intent classification, Guardrails
#     * Planning: Project/task planning, Multi-step workflows, Multi-turn conversations
#     * IDE: VSCode workflows, Git version control
#     * Project-specific: Mesosync workspace, AI/ML patterns
#     * Specialized: Advanced horticulture, Apothecary, Herbalism, Fermentation
#     * Context Switching: contextType: external (conversational) vs internal (JSON)
# - 10% strategic reasoning (Skein text adventures 14.6K)

# Context Switching Dataset (Manchurian Candidate training)
# This teaches the model to switch output modes based on contextType in system prompt:
#   - contextType: external → Natural conversational responses
#   - contextType: internal → Structured JSON for orchestrator
# Higher weight (3x) to ensure the signal is learned strongly
context_switching_weight: 3
