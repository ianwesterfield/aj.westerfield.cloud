# Adapter Merge Configuration for AJ
# Combines agentic (tool/reasoning) and conversational (personality/chat) adapters

base_model: "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"

adapters:
  # AJ v2.1.0 LoRA - mixed dataset (conversational + domain + strategic)
  # Trained with mixed_v1_h200.yaml on H200
  - path: "/mnt/c/Models/AJ-DeepSeekR1Qwen32B-v2.1.0/AJ-DeepSeekR1Qwen32B-v2.1.0-lora"
    weight: 1.0

# Merge strategy
method: "linear"    # "linear" or "ties"
# ties_density: 0.5  # Only used if method=ties

# Output
output: "./output/deepseek-r1-distill-qwen-32b-aj-merged"

# Post-merge conversion
convert_to_gguf: true
gguf_quantization: "Q8_0"  # Options: Q4_0, Q4_K_M, Q5_K_M, Q8_0, f16
