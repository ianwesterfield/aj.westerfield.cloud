# Adapter Merge Configuration for AJ - Q5_K_M (RTX 4090 Optimized)
# ~22GB model - fits in 24GB VRAM with room for KV cache
# Recommended for: RTX 4090, full GPU inference, no CPU offload

base_model: "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"

adapters:
  # AJ v2.1.0 LoRA - mixed dataset (conversational + domain + strategic)
  # Trained with mixed_v1_h200.yaml on H200
  - path: "/mnt/c/Models/AJ-DeepSeekR1Qwen32B-v2.1.0/AJ-DeepSeekR1Qwen32B-v2.1.0-lora"
    weight: 1.0

# Merge strategy
method: "linear"    # "linear" or "ties"
# ties_density: 0.5  # Only used if method=ties
offload_buffers: true

# Output
output: "./output/deepseek-r1-distill-qwen-32b-aj-Q5_K_M"

# Post-merge conversion
convert_to_gguf: true
gguf_quantization: "Q5_K_M"
