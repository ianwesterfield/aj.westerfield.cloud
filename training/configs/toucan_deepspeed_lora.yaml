model_name: "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"
output_dir: "./toucan-deepspeed-lora-output"

# Batch configuration
batch_size: 2  # Slightly higher with ZeRO-3 offloading
gradient_accumulation_steps: 4  # Effective batch = 2 * 4 GPUs * 4 = 32
num_epochs: 1
max_steps: 5000

# LoRA configuration
lora_r: 64
lora_alpha: 128
lora_dropout: 0.05
target_modules:
  - "q_proj"
  - "k_proj"
  - "v_proj"
  - "o_proj"
  - "gate_proj"
  - "up_proj"
  - "down_proj"

# Learning rate
learning_rate: 2e-5
warmup_steps: 500

# Optimization
weight_decay: 0.01
max_grad_norm: 1.0

# Sequence length
max_seq_length: 1024

# Logging
logging_steps: 10
save_steps: 500
eval_steps: 500
