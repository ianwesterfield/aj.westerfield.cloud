# SFT Training Configuration for Agentic Model
# Requires: 8xH100 80GB or equivalent (640GB total VRAM for 32B full fine-tune)

base_model: "Qwen/Qwen2.5-32B-Instruct"  # or "Qwen/Qwen2.5-72B-Instruct" for larger
output_dir: "./checkpoints/aj-agentic-32b-sft"

# Training approach
training_type: "full"  # full fine-tune, not LoRA
precision: "bf16"      # bfloat16 for H100s

# Data configuration
data:
  # Priority 1: Agentic trajectories (highest weight)
  trajectories:
    path: "data/trajectories/"
    weight: 3.0
    format: "trajectory"
  
  # Priority 2: Reasoning chains
  reasoning:
    path: "data/reasoning/"
    weight: 2.0
    format: "chatml"
  
  # Priority 3: Domain knowledge (your existing data)
  domain:
    path: "data/domain/"
    weight: 1.0
    format: "chatml"
  
  # Priority 4: Multi-turn conversations
  conversations:
    path: "data/conversations/"
    weight: 1.5
    format: "chatml"

# Training hyperparameters
training:
  epochs: 3
  batch_size: 128            # Global batch size across all GPUs
  micro_batch_size: 2        # Per-GPU batch size
  gradient_accumulation: 8   # 128 / (8 GPUs * 2) = 8
  
  learning_rate: 1.0e-5
  lr_scheduler: "cosine"
  warmup_ratio: 0.03
  weight_decay: 0.01
  
  max_seq_length: 8192       # Long context for trajectories
  
  # Optimization
  gradient_checkpointing: true
  flash_attention: true
  fsdp: true                 # Fully Sharded Data Parallel

# System prompt for training
system_prompt: |
  You are AJ, an expert AI coding assistant. You help with programming tasks by:
  1. Thinking through problems step by step
  2. Using tools when appropriate (read files, search code, run commands)
  3. Verifying your changes work correctly
  4. Explaining your reasoning clearly
  
  You have access to tools for reading/editing files, searching code, and running terminal commands.
  Use them thoughtfully to accomplish tasks efficiently.

# Evaluation during training
eval:
  eval_steps: 500
  eval_samples: 200
  metrics:
    - "loss"
    - "tool_use_accuracy"
    - "reasoning_coherence"

# Checkpointing
save:
  save_steps: 1000
  save_total_limit: 3
  push_to_hub: false

# Hardware expectations
hardware:
  expected_gpus: 8
  expected_vram_per_gpu: 80  # GB
  estimated_time_hours: 72   # ~3 days
